{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b4d380-a1dd-43c8-92b9-fa63baeec9cc",
   "metadata": {},
   "source": [
    "# <div align='center'><b>Introduction Summary for Python Web Scraping Files</b></div>\n",
    "> In this project for CSIS 4260 Assignment 2, I have chosen to compare two popular web scraping libraries: BeautifulSoup and Scrapy.\n",
    "> \n",
    ">  Both libraries are widely used for web scraping tasks due to their flexibility and ease of use.\n",
    ">\n",
    ">  I selected BBC News as the public website to scrape articles from, ensuring that more than 100 articles were collected.\n",
    ">\n",
    "> The BeautifulSoup implementation focuses on simplicity and quick extraction, while Scrapy provides a more structured and scalable approach.\n",
    ">\n",
    "> Timing benchmarks have been included in both implementations to compare the efficiency of each library.\n",
    ">\n",
    ">  The final result displays the total count of scraped articles and the time taken for the process.\n",
    ">\n",
    "> This project demonstrates the strengths of each library in handling web scraping tasks efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c139d2f-cd70-4dac-83df-05f21ee2455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed: 82 articles saved in 79.99 seconds using BeautifulSoup.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# BBC News URL\n",
    "BBC_URL = 'https://www.bbc.com/news'\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = requests.get(BBC_URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "articles = []\n",
    "for link in soup.find_all('a', href=True):\n",
    "    url = link['href']\n",
    "    if '/news/' in url and url.startswith('/news/'):\n",
    "        articles.append('https://www.bbc.com' + url)\n",
    "\n",
    "# Scrape articles details\n",
    "data = []\n",
    "for article_url in articles[:500]: \n",
    "    try:\n",
    "        article_response = requests.get(article_url, headers=HEADERS)\n",
    "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "\n",
    "        title = article_soup.find('h1').get_text() if article_soup.find('h1') else 'No title'\n",
    "        content = ' '.join([p.get_text() for p in article_soup.find_all('p')])\n",
    "\n",
    "        data.append({'title': title, 'url': article_url, 'content': content})\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "with open('bbc_news_articles_bs.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['title', 'url', 'content'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Scraping completed: {len(data)} articles saved in {end_time - start_time:.2f} seconds using BeautifulSoup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5a176c-f44a-4bd7-a73a-973374b6dded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed using Scrapy: 233 articles saved in 2.29 seconds.\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import time\n",
    "import csv\n",
    "\n",
    "class BBCNewsSpider(scrapy.Spider):\n",
    "    name = 'bbc_news'\n",
    "    start_urls = ['https://www.bbc.com/news']\n",
    "\n",
    "    def parse(self, response):\n",
    "        article_links = response.css('a[href*=\"/news/\"]::attr(href)').getall()\n",
    "        article_links = list(set(['https://www.bbc.com' + link for link in article_links if link.startswith('/news/')]))\n",
    "        for link in article_links[:500]:  \n",
    "            yield scrapy.Request(url=link, callback=self.parse_article)\n",
    "\n",
    "    def parse_article(self, response):\n",
    "        title = response.css('h1::text').get() or 'No title'\n",
    "        content = ' '.join(response.css('p::text').getall())\n",
    "        yield {\n",
    "            'title': title,\n",
    "            'url': response.url,\n",
    "            'content': content\n",
    "        }\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    'FEEDS': {\n",
    "        'bbc_news_articles_scrapy.csv': {'format': 'csv'},\n",
    "    },\n",
    "    'LOG_ENABLED': False,\n",
    "})\n",
    "\n",
    "process.crawl(BBCNewsSpider)\n",
    "process.start()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Scraping completed using Scrapy: {sum(1 for line in open('bbc_news_articles_scrapy.csv', encoding='utf-8'))-1} articles saved in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d6792-461a-4724-bc2b-6839ac3947b1",
   "metadata": {},
   "source": [
    "### <div align='center'><h3>Comparison Table of BeautifulSoup and Scrapy Performance</h3></div>\n",
    "| **Feature**         | **BeautifulSoup**                                   | **Scrapy**                                            |\n",
    "|---------------------|------------------------------------------------------|-------------------------------------------------------|\n",
    "| Articles Scraped    | 82                                                   | 233                                                   |\n",
    "| Time Taken (s)      | 79.99                                                | 2.29                                                  |\n",
    "| Ease of Setup       | Easy to set up, minimal dependencies                 | Requires project structure and setup                  |\n",
    "| Speed               | Slower for large datasets                            | Faster with built-in asynchronous requests            |\n",
    "| Scalability         | Suitable for small to medium tasks                   | Highly scalable for large-scale scraping              |\n",
    "| Code Complexity     | Simpler code, manual handling of requests            | More complex, but automates crawling and data storage |\n",
    "| Built-in Features   | No built-in crawling, needs external handling        | Built-in support for crawling, pipelines, and more    |\n",
    "| Best For            | Small projects, quick tasks                          | Large projects with high data volume                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7be0a6-f95c-458d-bf3b-d4e802e269a0",
   "metadata": {},
   "source": [
    "### <div align='center'><h3>Key Differences</h3></div>\n",
    "> BeautifulSoup is lightweight and simpler to implement but slower for large-scale scraping.\n",
    "\n",
    "> Scrapy is a full-fledged framework with built-in crawling and faster performance.\n",
    "\n",
    "> BeautifulSoup requires manual handling of requests and parsing, while Scrapy automates these processes.\n",
    "\n",
    "> Scrapy supports asynchronous scraping, making it more efficient than BeautifulSoup for extensive data collection.\n",
    "\n",
    "> Scrapy provides better scalability and built-in tools for managing large datasets, while BeautifulSoup is ideal for quick and simple scraping tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
